import{_ as s,o as a,c as e,ag as i}from"./chunks/framework.DEqXEGcv.js";const c=JSON.parse('{"title":"2026年2月10日 Agent Memory 领域最新研究进展","description":"","frontmatter":{},"headers":[],"relativePath":"20260210_agent_memory.md","filePath":"20260210_agent_memory.md","lastUpdated":null}'),r={name:"20260210_agent_memory.md"};function l(t,n,o,p,g,m){return a(),e("div",null,[...n[0]||(n[0]=[i(`<h1 id="_2026年2月10日-agent-memory-领域最新研究进展" tabindex="-1">2026年2月10日 Agent Memory 领域最新研究进展 <a class="header-anchor" href="#_2026年2月10日-agent-memory-领域最新研究进展" aria-label="Permalink to &quot;2026年2月10日 Agent Memory 领域最新研究进展&quot;">​</a></h1><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[1] BudgetMem: Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</span></span>
<span class="line"><span>作者：Haozhen Zhang, Haodong Yue, Tao Feng et al. | 第一单位：清华大学等</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.06025</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对现有 agent memory 系统依赖离线、query-agnostic 内存构建导致的效率低下问题，提出 BudgetMem 框架，通过三层预算路由（Low/Mid/High）实现显式的 query-aware 性能-成本控制，在 LoCoMo、LongMemEval 和 HotpotQA 基准上超越强基线。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>离线内存构建</strong>：大多数现有系统在查询前静态构建 memory，可能丢弃关键信息</li><li><strong>运行时内存利用</strong>：虽然更灵活，但以往工作开销大且缺乏显式的性能-成本控制</li><li><strong>本工作差异</strong>：提出显式的 budget-tier routing 机制，通过轻量级神经网络策略动态选择 memory 模块的预算层级</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>BudgetMem 框架</strong>：将 memory 处理结构化为多个 memory 模块，每个模块提供三层预算（Low/Mid/High）</li><li><strong>轻量级路由策略</strong>：使用紧凑的神经网络策略进行 budget-tier routing，通过强化学习训练</li><li><strong>三层 tiering 策略</strong>：Implementation（方法复杂度）、Reasoning（推理行为）、Capacity（模块模型大小）</li><li><strong>统一测试平台</strong>：系统研究不同 tiering 策略在各种预算制度下的 trade-offs</li></ol><p><strong>效果</strong>：</p><ul><li><strong>高预算设置</strong>：在 LoCoMo、LongMemEval、HotpotQA 上超越强基线</li><li><strong>成本-准确率前沿</strong>：在 tighter budgets 下实现更好的准确率-成本权衡</li><li><strong>策略分析</strong>：澄清了不同 tiering 策略在 varying budget regimes 下的优劣势</li></ul><p><strong>代码：</strong> <a href="https://github.com/ViktorAxelsen/BudgetMem" target="_blank" rel="noreferrer">https://github.com/ViktorAxelsen/BudgetMem</a></p></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[2] Learning to Share (LTS): Selective Memory for Efficient Parallel Agentic Systems</span></span>
<span class="line"><span>作者：Joseph Fioresi, Parth Parag Kulkarni, Ashmal Vayani et al. | 第一单位：University of Central Florida</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.05965</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对并行 agentic 系统中多团队独立推理导致的大量重复计算问题，提出 Learning to Share (LTS) 框架，通过学习的选择性共享 memory 机制实现跨团队信息复用，在 AssistantBench 和 GAIA 基准上显著降低运行时间同时保持或提升任务性能。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>并行 agentic 系统</strong>：部署多团队并行探索 diverse reasoning trajectories 以提高鲁棒性和解决方案质量</li><li><strong>计算成本问题</strong>：不同团队独立推理相似子问题时产生大量重叠计算</li><li><strong>本工作差异</strong>：引入全局共享 memory bank 和轻量级控制器，实现选择性跨团队信息复用</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>全局共享 Memory Bank</strong>：所有团队可访问的共享 memory 机制</li><li><strong>轻量级 Memory 控制器</strong>：决定 agent 的中间步骤是否应该加入 memory</li><li><strong>Stepwise RL 训练</strong>：使用 usage-aware credit assignment 的逐步强化学习训练控制器</li><li><strong>信息全局有用性识别</strong>：控制器能够识别在并行执行中全局有用的信息</li></ol><p><strong>效果</strong>：</p><ul><li><strong>效率提升</strong>：显著降低整体运行时间</li><li><strong>性能保持</strong>：匹配或超越无 memory 并行基线的任务性能</li><li><strong>基准测试</strong>：在 AssistantBench 和 GAIA 上验证有效</li></ul><p><strong>项目页：</strong> <a href="https://joefioresi718.github.io/LTS_webpage/" target="_blank" rel="noreferrer">https://joefioresi718.github.io/LTS_webpage/</a></p></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[3] Graph-based Agent Memory: Taxonomy, Techniques, and Applications</span></span>
<span class="line"><span>作者：Chang Yang, Chuang Zhou, Yilin Xiao et al. | 第一单位：厦门大学、香港理工大学等</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.05665</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：首篇从 graph-based 视角系统性综述 agent memory 的论文，提出完整的 taxonomy（短期 vs 长期、知识 vs 经验、非结构 vs 结构），覆盖 memory 生命周期的关键技术（提取、存储、检索、演化），并总结了开源库、benchmarks 及未来研究方向。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>Agent memory 重要性</strong>：memory 是 LLM-based agents 处理长程复杂任务的核心模块</li><li><strong>Graph 结构优势</strong>：能够建模关系依赖、组织层次信息、支持高效检索</li><li><strong>本工作差异</strong>：首次从 graph-based 视角全面综述 agent memory，提供 taxonomy 和关键技术系统分析</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>Taxonomy 分类框架</strong>： <ul><li>短期 vs 长期 memory</li><li>知识 vs 经验 memory</li><li>非结构 vs 结构 memory</li></ul></li><li><strong>Memory 生命周期技术</strong>： <ul><li><strong>Memory Extraction</strong>：将数据转换为 memory 内容</li><li><strong>Storage</strong>：高效组织数据</li><li><strong>Retrieval</strong>：从 memory 检索相关内容支持推理</li><li><strong>Evolution</strong>：更新 memory 内容实现自我进化</li></ul></li><li><strong>开源资源总结</strong>：收集相关研究论文、开源数据和项目</li><li><strong>未来方向</strong>：识别关键挑战和研究机遇</li></ol><p><strong>效果</strong>：</p><ul><li><strong>系统性</strong>：首次全面综述 graph-based agent memory</li><li><strong>实用性</strong>：提供 actionable insights 促进高效可靠的 memory 系统开发</li><li><strong>资源聚合</strong>：GitHub 收集相关资源 <a href="https://github.com/DEEP-PolyU/Awesome-GraphMemory" target="_blank" rel="noreferrer">https://github.com/DEEP-PolyU/Awesome-GraphMemory</a></li></ul></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[4] UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents</span></span>
<span class="line"><span>作者：Han Xiao, Guozhi Wang, Hao Wang et al. | 第一单位：未明确</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.05832</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对在线 RL 在 GUI agent 中的信用分配效率低和跨任务重复错误问题，提出 UI-Mem 框架，引入层次化经验记忆积累结构化知识（工作流、子任务技能、失败模式），支持跨任务和跨应用迁移，通过 Stratified Group Sampling 和 Self-Evolving Loop 实现 memory 与策略的持续对齐。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>在线 RL for GUI Agents</strong>：通过环境交互增强 GUI agent，但信用分配效率低</li><li><strong>经验迁移缺失</strong>：缺乏经验迁移导致跨任务重复错误</li><li><strong>传统 Replay Buffer</strong>：与 UI-Mem 的层次化经验记忆形成对比</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>层次化经验记忆 (Hierarchical Experience Memory)</strong>： <ul><li>积累工作流、子任务技能、失败模式</li><li>参数化模板存储，支持跨任务和跨应用迁移</li></ul></li><li><strong>Stratified Group Sampling</strong>： <ul><li>在每个 rollout group 的不同轨迹中注入不同级别的 guidance</li><li>保持 outcome diversity，驱动无指导策略内化指导行为</li></ul></li><li><strong>Self-Evolving Loop</strong>： <ul><li>持续抽象新策略和错误</li><li>保持 memory 与 agent 进化策略的对齐</li></ul></li></ol><p><strong>效果</strong>：</p><ul><li><strong>显著超越</strong>：显著超越传统 RL 基线和静态复用策略</li><li><strong>强泛化性</strong>：在未见过的应用上表现良好</li><li><strong>项目页：</strong> <a href="https://ui-mem.github.io" target="_blank" rel="noreferrer">https://ui-mem.github.io</a></li></ul></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[5] InfMem: Learning System-2 Memory Control for Long-Context Agent</span></span>
<span class="line"><span>作者：Xinyu Wang, Mingze Li, Peng Lu et al. | 第一单位：McGill University</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.02704</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对超长文档（32k-1M tokens）中稀疏证据的多跳推理问题，提出 InfMem 控制中心 agent，通过 PreThink-Retrieve-Write 协议实现 System-2 风格的主动记忆管理，在保持有界内存的同时显著提升推理准确率并降低推理成本。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>Streaming agents</strong>：采用被动内存更新策略，难以保留多跳推理所需的低显著性桥接证据</li><li><strong>MemAgent</strong>：作为主要对比基线</li><li><strong>本工作差异</strong>：从被动流式处理转向主动控制式内存管理，引入认知科学 System-2 理论</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>PreThink-Retrieve-Write 协议</strong>： <ul><li>PreThink 阶段：监控当前内存是否足以回答问题，若不足则合成问题条件化的检索查询并预测检索规模</li><li>Retrieve 阶段：对整个文档执行定向查询，支持非单调访问相关片段</li><li>Write 阶段：将当前片段与检索证据联合整合到有界覆写内存中</li></ul></li><li><strong>自适应早停机制</strong>：一旦在内存中巩固足够证据即终止检索-写入循环</li><li><strong>SFT-to-RL 训练方法</strong>：先用推理正确轨迹进行监督微调预热，再应用基于验证器的强化学习</li></ol><p><strong>效果</strong>：</p><ul><li><strong>准确率提升</strong>：在 32k-1M token 超长 QA 基准(LongBench QA)上显著提升 <ul><li>Qwen3-1.7B：平均绝对准确率 +10.17 分</li><li>Qwen3-4B：平均绝对准确率 +11.84 分</li><li>Qwen2.5-7B：平均绝对准确率 +8.23 分</li></ul></li><li><strong>效率优化</strong>：通过自适应早停，推理时间平均减少 3.9 倍（最高达 5.1 倍）</li></ul></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[6] Self-Consolidation for Self-Evolving Agents</span></span>
<span class="line"><span>作者：Hongzhuo Yu, Fei Zhu, Guo-Sen Xie et al. | 第一单位：未明确</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.01966</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对现有 agent 系统作为静态系统缺乏终身交互进化能力的问题，提出自进化框架，引入对比反思策略总结错误模式和可复用洞察，通过自巩固机制将非参数化文本经验蒸馏为紧凑可学习参数，实现 agent 的长期进化。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>静态 Agent 系统</strong>：现有 LLM agents 通常作为静态系统运行</li><li><strong>轨迹检索方法</strong>：主要依赖检索成功的过去轨迹作为演示，但忽略了失败尝试的教学价值</li><li><strong>文本经验累积问题</strong>：持续累积文本经验增加检索时间并引入噪声</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>对比反思策略 (Contrastive Reflection)</strong>： <ul><li>显式总结错误易发模式</li><li>捕获可复用的洞察</li></ul></li><li><strong>自巩固机制 (Self-Consolidation)</strong>： <ul><li>将非参数化文本经验蒸馏为紧凑可学习参数</li><li>使 agent 能够将大量历史经验内化到其潜在空间中</li></ul></li><li><strong>互补进化机制</strong>：对比反思 + 自巩固的双重机制</li></ol><p><strong>效果</strong>：</p><ul><li><strong>长期进化优势</strong>：在长期 agent 进化中展示优势</li><li><strong>克服遗忘</strong>：避免仅关注成功而忽略失败的问题</li><li><strong>克服上下文限制</strong>：解决文本经验累积导致的上下文窗口耗尽问题</li></ul></details>`,19)])])}const d=s(r,[["render",l]]);export{c as __pageData,d as default};
