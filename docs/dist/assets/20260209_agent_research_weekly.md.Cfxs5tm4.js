import{_ as s,o as a,c as e,ag as t}from"./chunks/framework.DEqXEGcv.js";const d=JSON.parse('{"title":"Agent Research Weekly - 2026-02-09","description":"","frontmatter":{},"headers":[],"relativePath":"20260209_agent_research_weekly.md","filePath":"20260209_agent_research_weekly.md","lastUpdated":null}'),l={name:"20260209_agent_research_weekly.md"};function i(r,n,o,p,g,u){return a(),e("div",null,[...n[0]||(n[0]=[t(`<h1 id="agent-research-weekly-2026-02-09" tabindex="-1">Agent Research Weekly - 2026-02-09 <a class="header-anchor" href="#agent-research-weekly-2026-02-09" aria-label="Permalink to &quot;Agent Research Weekly - 2026-02-09&quot;">​</a></h1><blockquote><p>本周聚焦：Agent Memory、Long-Horizon Agent、Context Retrieval &amp; Long Context 筛选范围：2026-02-02 至 2026-02-09 | 排除多模态论文</p></blockquote><hr><h2 id="一、agent-memory-研究进展" tabindex="-1">一、Agent Memory 研究进展 <a class="header-anchor" href="#一、agent-memory-研究进展" aria-label="Permalink to &quot;一、Agent Memory 研究进展&quot;">​</a></h2><h3 id="领域趋势洞察" tabindex="-1">领域趋势洞察 <a class="header-anchor" href="#领域趋势洞察" aria-label="Permalink to &quot;领域趋势洞察&quot;">​</a></h3><p>本周 Agent Memory 研究呈现<strong>三大主线</strong>：(1) <strong>Runtime Query-Aware Memory</strong> 成为热点，BudgetMem 等工作探索显式的性能-成本控制机制；(2) <strong>跨 Agent/团队 Memory 共享</strong> 受到关注，Learning to Share 提出了并行系统中的选择性信息共享；(3) <strong>Graph-Based Memory 系统化</strong>，首篇全面综述从图视角梳理了 memory 的 taxonomy 和生命周期技术。此外，<strong>经验积累与持续进化</strong>也是重要方向，Self-Consolidation 和 UI-Mem 等框架探索 agent 的自我进化能力。</p><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[1] BudgetMem: Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</span></span>
<span class="line"><span>作者：Haozhen Zhang, Haodong Yue, Tao Feng et al. | 第一单位：清华大学等</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.06025</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对现有 agent memory 系统依赖离线、query-agnostic 内存构建导致的效率低下问题，提出 BudgetMem 框架，通过三层预算路由（Low/Mid/High）实现显式的 query-aware 性能-成本控制，在 LoCoMo、LongMemEval 和 HotpotQA 基准上超越强基线。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>离线内存构建</strong>：大多数现有系统在查询前静态构建 memory，可能丢弃关键信息</li><li><strong>运行时内存利用</strong>：虽然更灵活，但以往工作开销大且缺乏显式的性能-成本控制</li><li><strong>本工作差异</strong>：提出显式的 budget-tier routing 机制，通过轻量级神经网络策略动态选择 memory 模块的预算层级</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>BudgetMem 框架</strong>：将 memory 处理结构化为多个 memory 模块，每个模块提供三层预算（Low/Mid/High）</li><li><strong>轻量级路由策略</strong>：使用紧凑的神经网络策略进行 budget-tier routing，通过强化学习训练</li><li><strong>三层 tiering 策略</strong>：Implementation（方法复杂度）、Reasoning（推理行为）、Capacity（模块模型大小）</li><li><strong>统一测试平台</strong>：系统研究不同 tiering 策略在各种预算制度下的 trade-offs</li></ol><p><strong>效果</strong>：</p><ul><li><strong>高预算设置</strong>：在 LoCoMo、LongMemEval、HotpotQA 上超越强基线</li><li><strong>成本-准确率前沿</strong>：在 tighter budgets 下实现更好的准确率-成本权衡</li><li><strong>策略分析</strong>：澄清了不同 tiering 策略在 varying budget regimes 下的优劣势</li></ul><p><strong>代码：</strong> <a href="https://github.com/ViktorAxelsen/BudgetMem" target="_blank" rel="noreferrer">https://github.com/ViktorAxelsen/BudgetMem</a></p></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[2] Learning to Share (LTS): Selective Memory for Efficient Parallel Agentic Systems</span></span>
<span class="line"><span>作者：Joseph Fioresi, Parth Parag Kulkarni, Ashmal Vayani et al. | 第一单位：University of Central Florida</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.05965</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对并行 agentic 系统中多团队独立推理导致的大量重复计算问题，提出 Learning to Share (LTS) 框架，通过学习的选择性共享 memory 机制实现跨团队信息复用，在 AssistantBench 和 GAIA 基准上显著降低运行时间同时保持或提升任务性能。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>并行 agentic 系统</strong>：部署多团队并行探索 diverse reasoning trajectories 以提高鲁棒性和解决方案质量</li><li><strong>计算成本问题</strong>：不同团队独立推理相似子问题时产生大量重叠计算</li><li><strong>本工作差异</strong>：引入全局共享 memory bank 和轻量级控制器，实现选择性跨团队信息复用</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>全局共享 Memory Bank</strong>：所有团队可访问的共享 memory 机制</li><li><strong>轻量级 Memory 控制器</strong>：决定 agent 的中间步骤是否应该加入 memory</li><li><strong>Stepwise RL 训练</strong>：使用 usage-aware credit assignment 的逐步强化学习训练控制器</li><li><strong>信息全局有用性识别</strong>：控制器能够识别在并行执行中全局有用的信息</li></ol><p><strong>效果</strong>：</p><ul><li><strong>效率提升</strong>：显著降低整体运行时间</li><li><strong>性能保持</strong>：匹配或超越无 memory 并行基线的任务性能</li><li><strong>基准测试</strong>：在 AssistantBench 和 GAIA 上验证有效</li></ul><p><strong>项目页：</strong> <a href="https://joefioresi718.github.io/LTS_webpage/" target="_blank" rel="noreferrer">https://joefioresi718.github.io/LTS_webpage/</a></p></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[3] Graph-based Agent Memory: Taxonomy, Techniques, and Applications</span></span>
<span class="line"><span>作者：Chang Yang, Chuang Zhou, Yilin Xiao et al. | 第一单位：厦门大学、香港理工大学等</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.05665</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：首篇从 graph-based 视角系统性综述 agent memory 的论文，提出完整的 taxonomy（短期 vs 长期、知识 vs 经验、非结构 vs 结构），覆盖 memory 生命周期的关键技术（提取、存储、检索、演化），并总结了开源库、benchmarks 及未来研究方向。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>Agent memory 重要性</strong>：memory 是 LLM-based agents 处理长程复杂任务的核心模块</li><li><strong>Graph 结构优势</strong>：能够建模关系依赖、组织层次信息、支持高效检索</li><li><strong>本工作差异</strong>：首次从 graph-based 视角全面综述 agent memory，提供 taxonomy 和关键技术系统分析</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>Taxonomy 分类框架</strong>： <ul><li>短期 vs 长期 memory</li><li>知识 vs 经验 memory</li><li>非结构 vs 结构 memory</li></ul></li><li><strong>Memory 生命周期技术</strong>： <ul><li><strong>Memory Extraction</strong>：将数据转换为 memory 内容</li><li><strong>Storage</strong>：高效组织数据</li><li><strong>Retrieval</strong>：从 memory 检索相关内容支持推理</li><li><strong>Evolution</strong>：更新 memory 内容实现自我进化</li></ul></li><li><strong>开源资源总结</strong>：收集相关研究论文、开源数据和项目</li><li><strong>未来方向</strong>：识别关键挑战和研究机遇</li></ol><p><strong>效果</strong>：</p><ul><li><strong>系统性</strong>：首次全面综述 graph-based agent memory</li><li><strong>实用性</strong>：提供 actionable insights 促进高效可靠的 memory 系统开发</li><li><strong>资源聚合</strong>：GitHub 收集相关资源 <a href="https://github.com/DEEP-PolyU/Awesome-GraphMemory" target="_blank" rel="noreferrer">https://github.com/DEEP-PolyU/Awesome-GraphMemory</a></li></ul></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[4] UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents</span></span>
<span class="line"><span>作者：Han Xiao, Guozhi Wang, Hao Wang et al. | 第一单位：未明确</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.05832</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对在线 RL 在 GUI agent 中的信用分配效率低和跨任务重复错误问题，提出 UI-Mem 框架，引入层次化经验记忆积累结构化知识（工作流、子任务技能、失败模式），支持跨任务和跨应用迁移，通过 Stratified Group Sampling 和 Self-Evolving Loop 实现 memory 与策略的持续对齐。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>在线 RL for GUI Agents</strong>：通过环境交互增强 GUI agent，但信用分配效率低</li><li><strong>经验迁移缺失</strong>：缺乏经验迁移导致跨任务重复错误</li><li><strong>传统 Replay Buffer</strong>：与 UI-Mem 的层次化经验记忆形成对比</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>层次化经验记忆 (Hierarchical Experience Memory)</strong>： <ul><li>积累工作流、子任务技能、失败模式</li><li>参数化模板存储，支持跨任务和跨应用迁移</li></ul></li><li><strong>Stratified Group Sampling</strong>： <ul><li>在每个 rollout group 的不同轨迹中注入不同级别的 guidance</li><li>保持 outcome diversity，驱动无指导策略内化指导行为</li></ul></li><li><strong>Self-Evolving Loop</strong>： <ul><li>持续抽象新策略和错误</li><li>保持 memory 与 agent 进化策略的对齐</li></ul></li></ol><p><strong>效果</strong>：</p><ul><li><strong>显著超越</strong>：显著超越传统 RL 基线和静态复用策略</li><li><strong>强泛化性</strong>：在未见过的应用上表现良好</li><li><strong>项目页：</strong> <a href="https://ui-mem.github.io" target="_blank" rel="noreferrer">https://ui-mem.github.io</a></li></ul></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[5] InfMem: Learning System-2 Memory Control for Long-Context Agent</span></span>
<span class="line"><span>作者：Xinyu Wang, Mingze Li, Peng Lu et al. | 第一单位：McGill University</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.02704</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对超长文档（32k-1M tokens）中稀疏证据的多跳推理问题，提出 InfMem 控制中心 agent，通过 PreThink-Retrieve-Write 协议实现 System-2 风格的主动记忆管理，在保持有界内存的同时显著提升推理准确率并降低推理成本。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>Streaming agents</strong>：采用被动内存更新策略，难以保留多跳推理所需的低显著性桥接证据</li><li><strong>MemAgent</strong>：作为主要对比基线</li><li><strong>本工作差异</strong>：从被动流式处理转向主动控制式内存管理，引入认知科学 System-2 理论</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>PreThink-Retrieve-Write 协议</strong>： <ul><li>PreThink 阶段：监控当前内存是否足以回答问题，若不足则合成问题条件化的检索查询并预测检索规模</li><li>Retrieve 阶段：对整个文档执行定向查询，支持非单调访问相关片段</li><li>Write 阶段：将当前片段与检索证据联合整合到有界覆写内存中</li></ul></li><li><strong>自适应早停机制</strong>：一旦在内存中巩固足够证据即终止检索-写入循环</li><li><strong>SFT-to-RL 训练方法</strong>：先用推理正确轨迹进行监督微调预热，再应用基于验证器的强化学习</li></ol><p><strong>效果</strong>：</p><ul><li><strong>准确率提升</strong>：在 32k-1M token 超长 QA 基准(LongBench QA)上显著提升 <ul><li>Qwen3-1.7B：平均绝对准确率 +10.17 分</li><li>Qwen3-4B：平均绝对准确率 +11.84 分</li><li>Qwen2.5-7B：平均绝对准确率 +8.23 分</li></ul></li><li><strong>效率优化</strong>：通过自适应早停，推理时间平均减少 3.9 倍（最高达 5.1 倍）</li></ul></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[6] Self-Consolidation for Self-Evolving Agents</span></span>
<span class="line"><span>作者：Hongzhuo Yu, Fei Zhu, Guo-Sen Xie et al. | 第一单位：未明确</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.01966</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对现有 agent 系统作为静态系统缺乏终身交互进化能力的问题，提出自进化框架，引入对比反思策略总结错误模式和可复用洞察，通过自巩固机制将非参数化文本经验蒸馏为紧凑可学习参数，实现 agent 的长期进化。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>静态 Agent 系统</strong>：现有 LLM agents 通常作为静态系统运行</li><li><strong>轨迹检索方法</strong>：主要依赖检索成功的过去轨迹作为演示，但忽略了失败尝试的教学价值</li><li><strong>文本经验累积问题</strong>：持续累积文本经验增加检索时间并引入噪声</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>对比反思策略 (Contrastive Reflection)</strong>： <ul><li>显式总结错误易发模式</li><li>捕获可复用的洞察</li></ul></li><li><strong>自巩固机制 (Self-Consolidation)</strong>： <ul><li>将非参数化文本经验蒸馏为紧凑可学习参数</li><li>使 agent 能够将大量历史经验内化到其潜在空间中</li></ul></li><li><strong>互补进化机制</strong>：对比反思 + 自巩固的双重机制</li></ol><p><strong>效果</strong>：</p><ul><li><strong>长期进化优势</strong>：在长期 agent 进化中展示优势</li><li><strong>克服遗忘</strong>：避免仅关注成功而忽略失败的问题</li><li><strong>克服上下文限制</strong>：解决文本经验累积导致的上下文窗口耗尽问题</li></ul></details><hr><h2 id="二、long-horizon-agent-研究进展" tabindex="-1">二、Long-Horizon Agent 研究进展 <a class="header-anchor" href="#二、long-horizon-agent-研究进展" aria-label="Permalink to &quot;二、Long-Horizon Agent 研究进展&quot;">​</a></h2><h3 id="领域趋势洞察-1" tabindex="-1">领域趋势洞察 <a class="header-anchor" href="#领域趋势洞察-1" aria-label="Permalink to &quot;领域趋势洞察&quot;">​</a></h3><p>本周 Long-Horizon Agent 研究聚焦于<strong>结构化规划</strong>和<strong>边缘设备部署</strong>两大方向。Table-as-Search 将长程信息搜索重构为表格补全任务，通过结构化状态管理提升长程探索的稳定性。AgentCPM-Explore 则突破了边缘 scale 模型的长程探索能力，证明 4B 参数模型通过系统训练框架可以达到 8B 甚至更大模型的性能。此外，OdysseyArena 等基准测试推动了对 agent 归纳能力和极端长程交互的评估。ProAct 和 Empirical-MCTS 则从训练范式角度探索如何让 agent 内化前瞻推理能力。</p><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[7] Table-as-Search (TaS): Formulate Long-Horizon Agentic Information Seeking as Table Completion</span></span>
<span class="line"><span>作者：Tian Lan, Felix Henry, Bin Zhu et al. | 第一单位：Alibaba International Digital Commerce</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.06724</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对长程信息搜索中状态追踪困难的问题，提出 Table-as-Search 框架，将 InfoSeeking 任务重构为表格补全任务，通过结构化表模式管理搜索状态，统一 Deep Search、Wide Search 和 DeepWide Search，在长程 InfoSeeking 中展示出优越的鲁棒性、效率和可扩展性。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>长程 InfoSeeking 挑战</strong>：在纯文本上下文中追踪搜索状态（规划过程和大量搜索结果）本质上很脆弱</li><li><strong>现有 agent 困境</strong>：难以在长程探索中保持专注和连贯性</li><li><strong>本工作差异</strong>：将任务重构为结构化表格补全，而非自由文本生成</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>表格化状态管理</strong>： <ul><li>将查询映射到外部数据库中维护的结构化表模式</li><li>行表示搜索候选，列表示约束或所需信息</li></ul></li><li><strong>双重功能设计</strong>： <ul><li>已填充单元格：严格记录历史和搜索结果</li><li>空单元格：作为显式搜索计划</li></ul></li><li><strong>统一三种 InfoSeeking 任务</strong>：Deep Search、Wide Search、DeepWide Search</li><li><strong>结构化规划框架</strong>：提供显式、可解释的搜索状态管理</li></ol><p><strong>效果</strong>：</p><ul><li><strong>显著超越</strong>：显著优于多种 SOTA 基线（包括多 agent 框架和商业系统）</li><li><strong>长程鲁棒性</strong>：在长程 InfoSeeking 中展示优越鲁棒性</li><li><strong>效率与可扩展性</strong>：高效且可扩展</li></ul><p><strong>代码：</strong> <a href="https://github.com/AIDC-AI/Marco-Search-Agent" target="_blank" rel="noreferrer">https://github.com/AIDC-AI/Marco-Search-Agent</a></p></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[8] AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents</span></span>
<span class="line"><span>作者：Haotian Chen, Xin Cong, Shengda Fan et al. | 第一单位：清华大学等</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.06485</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：首个针对 4B 参数规模边缘 agent 模型的系统性研究，识别出阻碍边缘模型性能的三大瓶颈（SFT 灾难性遗忘、RL 奖励噪声敏感、长上下文冗余信息），提出 holistic 训练框架（参数空间模型融合、奖励信号去噪、上下文信息精炼），在 GAIA 文本任务上达到 97.09% 准确率，匹配或超越 8B 模型。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>大模型依赖</strong>：现有系统严重依赖大规模模型，边缘 scale 模型能力未被充分探索</li><li><strong>边缘模型瓶颈</strong>：SFT 灾难性遗忘、RL 奖励噪声敏感、长上下文冗余信息导致的推理退化</li><li><strong>本工作差异</strong>：首个系统研究 4B 参数规模 agent 模型的训练方法</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>参数空间模型融合 (Parameter-Space Model Fusion)</strong>：解决 SFT 灾难性遗忘</li><li><strong>奖励信号去噪 (Reward Signal Denoising)</strong>：降低 RL 对奖励噪声的敏感性</li><li><strong>上下文信息精炼 (Contextual Information Refinement)</strong>：处理长上下文冗余信息</li><li><strong>Holistic 训练框架</strong>：整合上述三种技术的完整训练流程</li></ol><p><strong>效果</strong>：</p><ul><li><strong>SOTA 性能</strong>：在 4B 类模型中达到 SOTA</li><li><strong>超越 8B 模型</strong>：在四个基准上匹配或超越 8B 类 SOTA 模型</li><li><strong>超越大模型</strong>：在五个基准上超越 Claude-4.5-Sonnet、DeepSeek-v3.2 等更大模型</li><li><strong>GAIA 表现</strong>：在 GAIA 文本任务上达到 97.09% 准确率 (pass@64)</li><li><strong>核心结论</strong>：边缘模型的瓶颈不是固有能力上限，而是推理稳定性</li></ul></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[9] OdysseyArena: Benchmarking LLMs for Long-Horizon, Active and Inductive Interactions</span></span>
<span class="line"><span>作者：Fangzhi Xu, Hang Yan, Qiushi Sun et al. | 第一单位：复旦大学等</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.05843</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：现有评估主要采用演绎范式，忽略了 agent 从经验中自主发现潜在转移规律的归纳能力。OdysseyArena 重新聚焦于长程、主动、归纳交互，形式化并实例化四个原语，提供 120 个标准化任务（OdysseyArena-Lite）和极端交互场景（&gt;200步，OdysseyArena-Challenge），揭示即使前沿模型在归纳场景中也存在缺陷。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>演绎范式局限</strong>：现有评估基于显式规则和静态目标，规划范围有限</li><li><strong>归纳能力缺失</strong>：忽视了 agent 自主发现潜在转移规律的能力</li><li><strong>本工作差异</strong>：首次系统评估 agent 的归纳效率和长程发现能力</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>归纳交互形式化</strong>：形式化并实例化四个原语，将抽象转移动态转化为具体交互环境</li><li><strong>标准化基准 (OdysseyArena-Lite)</strong>： <ul><li>120 个任务</li><li>测量 agent 的归纳效率和长程发现能力</li></ul></li><li><strong>极端挑战 (OdysseyArena-Challenge)</strong>： <ul><li>极端交互场景（&gt;200 步）</li><li>压力测试 agent 稳定性</li></ul></li><li><strong>统一评估框架</strong>：系统评估 15+ 领先 LLM</li></ol><p><strong>效果</strong>：</p><ul><li><strong>发现瓶颈</strong>：即使前沿模型在归纳场景中也存在缺陷</li><li><strong>识别关键挑战</strong>：发现复杂环境中自主发现的关键瓶颈</li><li><strong>推动研究</strong>：为长程归纳交互研究提供标准化评估平台</li></ul><p><strong>代码：</strong> <a href="https://github.com/xufangzhi/Odyssey-Arena" target="_blank" rel="noreferrer">https://github.com/xufangzhi/Odyssey-Arena</a></p></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[10] ProAct: Agentic Lookahead in Interactive Environments</span></span>
<span class="line"><span>作者：Yangbin Yu, Mingyu Yang, Junyou Li et al. | 第一单位：未明确</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.05327</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对 LLM agents 在长程规划中的复合错误问题，提出 ProAct 框架，通过两阶段训练范式使 agent 内化准确的前瞻推理。Grounded LookAhead Distillation (GLAD) 将环境搜索轨迹压缩为简洁的因果推理链，Monte-Carlo Critic (MC-Critic) 通过轻量级环境 rollout 校准价值估计，4B 模型在 2048 和 Sokoban 上超越所有开源基线并匹敌 SOTA 闭源模型。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>长程规划挑战</strong>：模拟未来状态时的复合错误</li><li><strong>现有方法局限</strong>：缺乏有效的前瞻推理内化机制</li><li><strong>本工作差异</strong>：通过蒸馏将搜索能力转化为模型内在能力</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>Grounded LookAhead Distillation (GLAD)</strong>： <ul><li>基于环境搜索的轨迹进行监督微调</li><li>将复杂搜索树压缩为简洁因果推理链</li><li>学习预见逻辑而无需推理时搜索开销</li></ul></li><li><strong>Monte-Carlo Critic (MC-Critic)</strong>： <ul><li>即插即用的辅助价值估计器</li><li>利用轻量级环境 rollout 校准价值估计</li><li>为 PPO 和 GRPO 等策略梯度算法提供低方差信号</li></ul></li><li><strong>两阶段训练范式</strong>：GLAD + MC-Critic 的组合训练</li></ol><p><strong>效果</strong>：</p><ul><li><strong>规划准确率提升</strong>：在随机（2048）和确定性（Sokoban）环境中均显著提升</li><li><strong>4B 模型表现优异</strong>：超越所有开源基线，匹敌 SOTA 闭源模型</li><li><strong>强泛化性</strong>：在未见过环境上表现鲁棒</li></ul><p><strong>代码：</strong> <a href="https://github.com/GreatX3/ProAct" target="_blank" rel="noreferrer">https://github.com/GreatX3/ProAct</a></p></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[11] Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search</span></span>
<span class="line"><span>作者：Hao Lu, Haoyuan Huang, Yulin Zhou et al. | 第一单位：未明确</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.04248</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对现有 MCTS 方法无状态、丢弃成功推理模式的问题，提出 Empirical-MCTS 双循环框架，通过 Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) 和 Memory Optimization Agent 实现局部探索与全局 memory 优化的统一，在 AIME25、ARC-AGI-2 和 MathArena Apex 上显著超越无状态 MCTS 策略。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>Inference-time Scaling</strong>：MCTS 显著增强 LLM 推理能力</li><li><strong>现有 MCTS 局限</strong>：主要无状态，每个问题实例后丢弃成功推理模式</li><li><strong>人类问题解决</strong>：人类通过经验积累智慧，而非每次重新学习</li><li><strong>本工作差异</strong>：将无状态搜索转化为连续的、非参数化的学习过程</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>双循环框架 (Dual-Loop Framework)</strong>：统一局部探索与全局 memory 优化</li><li><strong>PE-EMP (Pairwise-Experience-Evolutionary Meta-Prompting)</strong>： <ul><li>在局部搜索中作为反射式优化器</li><li>使用成对反馈动态合成自适应标准</li><li>实时演化 meta-prompts (system prompts)</li></ul></li><li><strong>Memory Optimization Agent</strong>： <ul><li>管理全局 repository 作为动态 policy prior</li><li>使用原子操作跨问题蒸馏高质量洞察</li></ul></li><li><strong>经验积累机制</strong>：结构化搜索与经验积累耦合</li></ol><p><strong>效果</strong>：</p><ul><li><strong>AIME25</strong>：显著超越无状态 MCTS 策略</li><li><strong>ARC-AGI-2</strong>：超越独立经验驱动 agents</li><li><strong>MathArena Apex</strong>：在复杂开放推理任务上表现优异</li><li><strong>核心结论</strong>：结构化搜索与经验积累耦合对于掌握复杂开放推理任务至关重要</li></ul></details><hr><h2 id="三、context-retrieval-long-context-研究进展" tabindex="-1">三、Context Retrieval &amp; Long Context 研究进展 <a class="header-anchor" href="#三、context-retrieval-long-context-研究进展" aria-label="Permalink to &quot;三、Context Retrieval &amp; Long Context 研究进展&quot;">​</a></h2><h3 id="领域趋势洞察-2" tabindex="-1">领域趋势洞察 <a class="header-anchor" href="#领域趋势洞察-2" aria-label="Permalink to &quot;领域趋势洞察&quot;">​</a></h3><p>本周 Context Retrieval 和 Long Context 研究呈现<strong>三大趋势</strong>：(1) <strong>结构化文档理解</strong>成为焦点，DeepRead 利用文档原生结构（标题、段落边界）提升长文档问答；(2) <strong>Context 检索评估基准</strong>涌现，ContextBench 针对 coding agent 的上下文检索过程进行细粒度评估，CL-bench 则测试模型从复杂上下文中学习新知识的能力；(3) <strong>长上下文推理效率</strong>持续优化，LycheeDecode 通过混合头稀疏解码在 128K 上下文实现 2.7 倍加速。Bifrost 则探索了无需训练的上下文自适应方法。</p><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[12] DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search</span></span>
<span class="line"><span>作者：Zhanli Li, Huiwen Tian, Lvzhou Luo et al. | 第一单位：中国科学院等</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.05014</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对现有 agentic 搜索将长文档视为扁平 chunk 集合、未充分利用文档层级结构和顺序话语结构的问题，提出 DeepRead，通过 LLM-based OCR 将 PDF 转为保留结构的 Markdown，引入坐标式元数据和 Retrieve/ReadSection 工具，实现类似人类的&quot;定位-阅读&quot;行为范式。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>Agentic RAG 演进</strong>：从一次性被动检索向多轮决策驱动证据获取演进</li><li><strong>文档处理局限</strong>：现有框架将长文档视为扁平 chunk 集合</li><li><strong>本工作差异</strong>：显式利用文档原生先验（层级组织、顺序话语结构）</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>LLM-based OCR 转换</strong>：将 PDF 转为保留标题和段落边界的结构化 Markdown</li><li><strong>坐标式元数据</strong>：为每个段落分配编码章节身份和章节内顺序的坐标式元数据键</li><li><strong>双工具设计</strong>： <ul><li><strong>Retrieve 工具</strong>：定位相关段落并暴露其结构坐标（带轻量级扫描上下文）</li><li><strong>ReadSection 工具</strong>：在指定章节和段落范围内进行连续、保序阅读</li></ul></li><li><strong>&quot;定位-阅读&quot;范式</strong>：类似人类的文档阅读理解行为</li></ol><p><strong>效果</strong>：</p><ul><li><strong>显著提升</strong>：在文档问答中显著优于 Search-o1 风格的 agentic 搜索</li><li><strong>工具协同效应</strong>：检索和阅读工具之间的协同效应得到验证</li><li><strong>细粒度行为分析</strong>：揭示类似人类的&quot;定位-阅读&quot;行为模式</li></ul></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[13] ContextBench: A Benchmark for Context Retrieval in Coding Agents</span></span>
<span class="line"><span>作者：Han Li, Letian Zhu, Bohan Zhang et al. | 第一单位：未明确</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.05892</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：现有 coding agent 评估主要关注最终任务成功，缺乏对上下文检索过程的洞察。ContextBench 是面向 coding agent 上下文检索的过程导向评估基准，包含 1,136 个 issue-resolution 任务和人工标注的 gold contexts，测量上下文召回率、精确率和效率，揭示 agent scaffolding 在上下文检索中仅带来边际提升（&quot;The Bitter Lesson&quot;）。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>End-to-End 评估局限</strong>：现有评估关注最终成功率，忽略中间过程</li><li><strong>Context 检索黑盒</strong>：缺乏对 agent 如何检索和使用代码上下文的洞察</li><li><strong>本工作差异</strong>：过程导向评估，追踪 agent 轨迹并测量中间指标</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>大规模数据集</strong>： <ul><li>1,136 个 issue-resolution 任务</li><li>来自 66 个 repositories，覆盖 8 种编程语言</li><li>人工标注的 gold contexts</li></ul></li><li><strong>自动化评估框架</strong>： <ul><li>追踪 agent 轨迹</li><li>测量上下文召回率、精确率和效率</li></ul></li><li><strong>过程导向指标</strong>：提供中间 gold-context 指标，解构 issue-resolution 过程</li><li><strong>细粒度分析</strong>：揭示探索和利用上下文之间的差距</li></ol><p><strong>核心发现</strong>：</p><ul><li><strong>Bitter Lesson</strong>：复杂的 agent scaffolding 在上下文检索中仅带来边际提升</li><li><strong>Recall &gt; Precision</strong>：LLMs 一致偏好召回而非精确</li><li><strong>探索-利用差距</strong>：探索的上下文与利用的上下文之间存在显著差距</li></ul><p><strong>代码：</strong> <a href="https://cioutn.github.io/context-bench/" target="_blank" rel="noreferrer">https://cioutn.github.io/context-bench/</a></p></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[14] LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding</span></span>
<span class="line"><span>作者：Gang Lin, Dongfang Li, Zhuoen Chen et al. | 第一单位：未明确</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.04541</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对长上下文 LLM 推理中 KV cache 快速扩展导致的内存和延迟瓶颈，提出 LycheeDecode，通过细粒度混合头注意力机制和硬件高效的 top-k 选择策略，将注意力头分为检索头和稀疏头，在 128K 上下文长度实现 2.7 倍加速，生成质量匹敌甚至超越全注意力基线。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>KV Cache 瓶颈</strong>：长上下文推理中 KV cache 快速扩展</li><li><strong>粗粒度共享局限</strong>：跨层共享同一组关键 token 忽视注意力头的功能多样性</li><li><strong>本工作差异</strong>：细粒度混合头注意力，保留注意力头的功能多样性</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>HardKuma-based 机制</strong>： <ul><li>将注意力头分为检索头（动态识别关键 token）和稀疏头（复用关键 token）</li></ul></li><li><strong>硬件高效的 top-k 选择策略</strong>：确保高效计算</li><li><strong>细粒度策略</strong>：克服现有粗粒度方法的性能瓶颈</li><li><strong>功能多样性保留</strong>：不同注意力头承担不同功能角色</li></ol><p><strong>效果</strong>：</p><ul><li><strong>生成质量</strong>：在 Llama3 和 Qwen3 上，生成质量匹敌甚至超越全注意力基线</li><li><strong>显著加速</strong>：在 128K 上下文长度实现 2.7 倍加速</li><li><strong>多基准验证</strong>：在 LongBench、RULER（长上下文理解）和 AIME24、OlympiadBench（复杂推理）上验证</li></ul><p><strong>会议：</strong> ICLR 2026</p></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[15] Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents</span></span>
<span class="line"><span>作者：Quan M. Tran, Zhuo Huang, Wenbin Zhang et al. | 第一单位：未明确</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.05810</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：针对自改进 agent 在跨任务复用轨迹时的上下文不匹配问题，提出 Bifrost，利用上下文差异精确引导历史轨迹适应目标任务。发现上下文-轨迹相关性（上下文偏移与轨迹偏移高度平行），在表示层面使用 agent hidden states 进行轨迹自适应，无需训练即可实现有效复用。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>轨迹复用挑战</strong>：跨任务时存在上下文不匹配</li><li><strong>现有方法局限</strong>：要么丢弃轨迹，要么使用启发式操作，导致微调成本高或性能无保证</li><li><strong>本工作差异</strong>：无需训练，利用上下文差异进行精确的轨迹自适应</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>上下文-轨迹相关性发现</strong>：上下文偏移与轨迹偏移高度平行</li><li><strong>BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost)</strong>： <ul><li>利用上下文差异精确引导历史轨迹适应目标任务</li><li>缓解上下文偏移导致的错位</li></ul></li><li><strong>表示层面自适应</strong>： <ul><li>使用 agent hidden states 进行轨迹自适应</li><li>在共享空间中准确对齐目标上下文</li></ul></li><li><strong>无需训练</strong>：training-free 方法</li></ol><p><strong>效果</strong>：</p><ul><li><strong>一致超越</strong>：在多个基准上一致超越现有轨迹复用和微调自改进方法</li><li><strong>有效复用</strong>：即使存在显著上下文偏移，agent 也能有效利用过去经验</li></ul></details><hr><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[16] CL-bench: A Benchmark for Context Learning</span></span>
<span class="line"><span>作者：Shihan Dou, Ming Zhang, Zhangyue Yin et al. | 第一单位：复旦大学等</span></span>
<span class="line"><span>链接：https://arxiv.org/abs/2602.03587</span></span>
<span class="line"><span></span></span>
<span class="line"><span>简单介绍：现有 LMs 主要利用预训练知识进行推理，但现实世界任务更复杂且依赖上下文：模型必须从任务特定上下文中学习并利用预训练之外的新知识。CL-bench 是首个 context learning 基准，包含 500 个复杂上下文、1,899 个任务，要求模型学习新领域知识、规则系统、复杂流程和经验数据导出的规律，揭示前沿模型平均仅解决 17.2% 的任务。</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><details><summary>详细总结</summary><p><strong>相关工作</strong>：</p><ul><li><strong>预训练知识局限</strong>：现有 LMs 主要依赖预训练知识</li><li><strong>Context Learning 缺失</strong>：从上下文中学习并利用新知识的 capability 被 largely overlooked</li><li><strong>长上下文 vs Context Learning</strong>：长上下文任务主要测试检索或阅读理解，in-context learning 主要学习简单任务模式</li></ul><p><strong>创新点</strong>：</p><ol><li><strong>真实世界基准</strong>： <ul><li>500 个复杂上下文</li><li>1,899 个任务</li><li>31,607 个验证标准</li><li>由经验丰富的领域专家精心制作</li></ul></li><li><strong>多样化学习内容</strong>： <ul><li>新领域特定知识</li><li>规则系统</li><li>复杂流程</li><li>经验数据导出的规律</li></ul></li><li><strong>超越现有任务</strong>：远超长上下文任务和 in-context learning 任务的复杂度</li></ol><p><strong>核心发现</strong>：</p><ul><li><strong>模型表现不佳</strong>：10 个前沿 LM 平均仅解决 17.2% 的任务</li><li><strong>最佳模型局限</strong>：即使 GPT-5.1 也仅解决 23.7%</li><li><strong>Critical Bottleneck</strong>：Context learning 是处理真实世界复杂上下文依赖任务的关键瓶颈</li></ul><p><strong>意义</strong>：</p><ul><li><strong>新 capability 评估</strong>：首次系统评估 LMs 的 context learning 能力</li><li><strong>推动研究</strong>：为构建具有这一基础能力的 LMs 提供评估平台</li></ul></details><hr><h2 id="本周综合洞察" tabindex="-1">本周综合洞察 <a class="header-anchor" href="#本周综合洞察" aria-label="Permalink to &quot;本周综合洞察&quot;">​</a></h2><h3 id="研究趋势总览" tabindex="-1">研究趋势总览 <a class="header-anchor" href="#研究趋势总览" aria-label="Permalink to &quot;研究趋势总览&quot;">​</a></h3><table tabindex="0"><thead><tr><th>主题</th><th>核心趋势</th><th>代表论文</th></tr></thead><tbody><tr><td><strong>Agent Memory</strong></td><td>Runtime Query-Aware、跨 Agent 共享、Graph-Based、自进化</td><td>BudgetMem、LTS、Graph Survey、Self-Consolidation</td></tr><tr><td><strong>Long-Horizon Agent</strong></td><td>结构化规划、边缘 scale 探索、归纳能力评估、前瞻推理内化</td><td>Table-as-Search、AgentCPM-Explore、OdysseyArena、ProAct</td></tr><tr><td><strong>Context Retrieval</strong></td><td>结构化文档理解、过程导向评估、长上下文效率优化</td><td>DeepRead、ContextBench、LycheeDecode</td></tr></tbody></table><h3 id="关键趋势总结" tabindex="-1">关键趋势总结 <a class="header-anchor" href="#关键趋势总结" aria-label="Permalink to &quot;关键趋势总结&quot;">​</a></h3><ol><li><p><strong>从被动 Memory 到主动控制</strong>：BudgetMem 和 InfMem 展示了通过 query-aware routing 和 System-2 控制实现显式 memory 管理的新方向</p></li><li><p><strong>Memory 共享与协作</strong>：Learning to Share 提出并行 agentic 系统中的选择性 memory 共享，多 agent 协作中的信息复用成为热点</p></li><li><p><strong>边缘 Scale Agent 突破</strong>：AgentCPM-Explore 证明 4B 参数模型通过系统训练可以达到甚至超越大模型的长程探索能力</p></li><li><p><strong>结构化 Context 理解</strong>：DeepRead 和 Table-as-Search 展示了利用文档/任务结构提升长上下文理解的有效路径</p></li><li><p><strong>过程导向评估兴起</strong>：ContextBench 和 CL-bench 推动从 end-to-end 成功率向中间过程指标的评估转变</p></li><li><p><strong>经验积累与持续学习</strong>：Empirical-MCTS 和 Self-Consolidation 探索 agent 通过 memory 实现持续学习和进化的机制</p></li></ol><h3 id="技术演进方向" tabindex="-1">技术演进方向 <a class="header-anchor" href="#技术演进方向" aria-label="Permalink to &quot;技术演进方向&quot;">​</a></h3><ul><li><strong>Memory</strong>：静态 → 动态/runtime → 主动控制/query-aware</li><li><strong>Context</strong>：扁平 chunk → 结构化文档 → 层次化表示</li><li><strong>评估</strong>：End-to-end → 过程导向 → 细粒度中间指标</li><li><strong>效率</strong>：全注意力 → 稀疏解码 → 混合头机制</li><li><strong>Scale</strong>：大模型中心 → 边缘 scale 优化 → 效率与能力并重</li></ul><hr><p><em>Generated by EamonBot 🔥 | 2026-02-09</em></p>`,72)])])}const m=s(l,[["render",i]]);export{d as __pageData,m as default};
