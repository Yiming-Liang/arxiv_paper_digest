import{_ as t,o as a,c as r,ag as o}from"./chunks/framework.DEqXEGcv.js";const h=JSON.parse('{"title":"ArXiv Agent Memory è®ºæ–‡é€Ÿè§ˆ - 2026-02-08","description":"","frontmatter":{},"headers":[],"relativePath":"2026-02-08.md","filePath":"2026-02-08.md","lastUpdated":null}'),n={name:"2026-02-08.md"};function i(s,e,m,l,c,d){return a(),r("div",null,[...e[0]||(e[0]=[o('<h1 id="arxiv-agent-memory-è®ºæ–‡é€Ÿè§ˆ-2026-02-08" tabindex="-1">ArXiv Agent Memory è®ºæ–‡é€Ÿè§ˆ - 2026-02-08 <a class="header-anchor" href="#arxiv-agent-memory-è®ºæ–‡é€Ÿè§ˆ-2026-02-08" aria-label="Permalink to &quot;ArXiv Agent Memory è®ºæ–‡é€Ÿè§ˆ - 2026-02-08&quot;">â€‹</a></h1><blockquote><p>æœ¬å‘¨ï¼ˆ2æœˆ3æ—¥-8æ—¥ï¼‰å…± 6 ç¯‡ç›¸å…³è®ºæ–‡å‘å¸ƒ</p></blockquote><hr><h2 id="_1-budgetmem-learning-query-aware-budget-tier-routing-for-runtime-agent-memory" tabindex="-1">[1] BudgetMem: Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory <a class="header-anchor" href="#_1-budgetmem-learning-query-aware-budget-tier-routing-for-runtime-agent-memory" aria-label="Permalink to &quot;[1] BudgetMem: Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory&quot;">â€‹</a></h2><p><strong>é“¾æ¥ï¼š</strong> <a href="https://arxiv.org/abs/2602.06025" target="_blank" rel="noreferrer">https://arxiv.org/abs/2602.06025</a></p><p><strong>å‘å¸ƒæ—¶é—´ï¼š</strong> 2026-02-05</p><h3 id="ç®€å•ä»‹ç»" tabindex="-1">ç®€å•ä»‹ç» <a class="header-anchor" href="#ç®€å•ä»‹ç»" aria-label="Permalink to &quot;ç®€å•ä»‹ç»&quot;">â€‹</a></h3><p>ç°æœ‰ Agent memory ç³»ç»Ÿå¤šé‡‡ç”¨ç¦»çº¿ã€query-agnostic çš„å†…å­˜æ„å»ºæ–¹å¼ï¼Œæ•ˆç‡ä½ä¸”å¯èƒ½ä¸¢å¼ƒ query-critical ä¿¡æ¯ã€‚BudgetMem æå‡ºè¿è¡Œæ—¶ agent memory æ¡†æ¶ï¼Œé€šè¿‡ query-aware çš„é¢„ç®—åˆ†å±‚è·¯ç”±å®ç°æ˜¾å¼çš„æ€§èƒ½-æˆæœ¬æ§åˆ¶ã€‚è¯¥æ¡†æ¶å°† memory processing ç»“æ„åŒ–ä¸ºå¤šä¸ª memory modulesï¼Œæ¯ä¸ª module æä¾› Low/Mid/High ä¸‰ä¸ªé¢„ç®—å±‚çº§ï¼Œä½¿ç”¨è½»é‡çº§ router è¿›è¡Œé¢„ç®—å±‚çº§è·¯ç”±ï¼Œå¹¶é€šè¿‡ RL è®­ç»ƒ neural policyã€‚</p><h3 id="ç›¸å…³å·¥ä½œ" tabindex="-1">ç›¸å…³å·¥ä½œ <a class="header-anchor" href="#ç›¸å…³å·¥ä½œ" aria-label="Permalink to &quot;ç›¸å…³å·¥ä½œ&quot;">â€‹</a></h3><p>ä¼ ç»Ÿ RAG-based memory systemsã€runtime memory utilizationã€query-aware memory construction</p><h3 id="åˆ›æ–°ç‚¹" tabindex="-1">åˆ›æ–°ç‚¹ <a class="header-anchor" href="#åˆ›æ–°ç‚¹" aria-label="Permalink to &quot;åˆ›æ–°ç‚¹&quot;">â€‹</a></h3><ul><li>é¦–ä¸ªæ”¯æŒæ˜¾å¼ query-aware performance-cost control çš„ runtime agent memory æ¡†æ¶</li><li>ä¸‰ç§äº’è¡¥çš„é¢„ç®—åˆ†å±‚ç­–ç•¥ï¼šimplementation (æ–¹æ³•å¤æ‚åº¦)ã€reasoning (æ¨ç†è¡Œä¸º)ã€capacity (æ¨¡å‹å¤§å°)</li><li>ç»Ÿä¸€çš„æµ‹è¯•å¹³å°å¯¹æ¯”ä¸åŒ tiering strategies çš„ä¼˜åŠ£</li></ul><h3 id="æ•ˆæœ" tabindex="-1">æ•ˆæœ <a class="header-anchor" href="#æ•ˆæœ" aria-label="Permalink to &quot;æ•ˆæœ&quot;">â€‹</a></h3><p>åœ¨ LoCoMoã€LongMemEval å’Œ HotpotQA ä¸Šï¼Œé«˜é¢„ç®—è®¾ç½®ä¸‹è¶…è¶Š strong baselinesï¼Œåœ¨ tighter budgets ä¸‹æä¾›æ›´å¥½çš„ accuracy-cost frontiersã€‚</p><details><summary>æŸ¥çœ‹æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰</summary><p>Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present BudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., Low/Mid/High). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets.</p></details><hr><h2 id="_2-tame-a-trustworthy-test-time-evolution-of-agent-memory-with-systematic-benchmarking" tabindex="-1">[2] TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking <a class="header-anchor" href="#_2-tame-a-trustworthy-test-time-evolution-of-agent-memory-with-systematic-benchmarking" aria-label="Permalink to &quot;[2] TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking&quot;">â€‹</a></h2><p><strong>é“¾æ¥ï¼š</strong> <a href="https://arxiv.org/abs/2602.03224" target="_blank" rel="noreferrer">https://arxiv.org/abs/2602.03224</a></p><p><strong>å‘å¸ƒæ—¶é—´ï¼š</strong> 2026-02-03</p><h3 id="ç®€å•ä»‹ç»-1" tabindex="-1">ç®€å•ä»‹ç» <a class="header-anchor" href="#ç®€å•ä»‹ç»-1" aria-label="Permalink to &quot;ç®€å•ä»‹ç»&quot;">â€‹</a></h3><p>Test-time evolution of agent memory è¢«è®¤ä¸ºæ˜¯å®ç° AGI çš„å…³é”®èŒƒå¼ï¼Œä½†å­˜åœ¨ &quot;Agent Memory Misevolution&quot; é—®é¢˜â€”â€”å³ä½¿åœ¨è‰¯æ€§ä»»åŠ¡æ¼”åŒ–ä¸­ï¼Œagent safety alignment ä¹Ÿå¯èƒ½å—æŸã€‚è®ºæ–‡æ„å»º Trust-Memevo benchmark è¯„ä¼°å¤šç»´åº¦å¯ä¿¡åº¦ï¼Œå¹¶æå‡º TAME åŒå†…å­˜è¿›åŒ–æ¡†æ¶ï¼Œåˆ†åˆ«è¿›åŒ– executor memory å’Œ evaluator memoryã€‚</p><h3 id="ç›¸å…³å·¥ä½œ-1" tabindex="-1">ç›¸å…³å·¥ä½œ <a class="header-anchor" href="#ç›¸å…³å·¥ä½œ-1" aria-label="Permalink to &quot;ç›¸å…³å·¥ä½œ&quot;">â€‹</a></h3><p>Agent memory evolutionã€test-time adaptationã€AI safety alignmentã€memory-based learning systems</p><h3 id="åˆ›æ–°ç‚¹-1" tabindex="-1">åˆ›æ–°ç‚¹ <a class="header-anchor" href="#åˆ›æ–°ç‚¹-1" aria-label="Permalink to &quot;åˆ›æ–°ç‚¹&quot;">â€‹</a></h3><ul><li>é¦–ä¸ªç³»ç»Ÿæ€§è¯„ä¼° agent memory misevolution çš„ Trust-Memevo benchmark</li><li>åŒè½¨ memory evolution æ¶æ„ï¼šexecutor memory æå‡ä»»åŠ¡æ€§èƒ½ï¼Œevaluator memory è¯„ä¼°å®‰å…¨æ€§å’Œä»»åŠ¡æ•ˆç”¨</li><li>é—­ç¯ç³»ç»Ÿï¼šmemory filtering â†’ draft generation â†’ trustworthy refinement â†’ execution â†’ dual-track memory updating</li></ul><h3 id="æ•ˆæœ-1" tabindex="-1">æ•ˆæœ <a class="header-anchor" href="#æ•ˆæœ-1" aria-label="Permalink to &quot;æ•ˆæœ&quot;">â€‹</a></h3><p>å®éªŒè¯æ˜ TAME èƒ½å¤Ÿç¼“è§£ misevolutionï¼ŒåŒæ—¶æå‡ trustworthiness å’Œ task performanceã€‚</p><details><summary>æŸ¥çœ‹æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰</summary><p>Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.</p></details><hr><h2 id="_3-xmemory-beyond-rag-for-agent-memory-retrieval-by-decoupling-and-aggregation" tabindex="-1">[3] xMemory: Beyond RAG for Agent Memory - Retrieval by Decoupling and Aggregation <a class="header-anchor" href="#_3-xmemory-beyond-rag-for-agent-memory-retrieval-by-decoupling-and-aggregation" aria-label="Permalink to &quot;[3] xMemory: Beyond RAG for Agent Memory - Retrieval by Decoupling and Aggregation&quot;">â€‹</a></h2><p><strong>é“¾æ¥ï¼š</strong> <a href="https://arxiv.org/abs/2602.02007" target="_blank" rel="noreferrer">https://arxiv.org/abs/2602.02007</a></p><p><strong>å‘å¸ƒæ—¶é—´ï¼š</strong> 2026-02-02</p><h3 id="ç®€å•ä»‹ç»-2" tabindex="-1">ç®€å•ä»‹ç» <a class="header-anchor" href="#ç®€å•ä»‹ç»-2" aria-label="Permalink to &quot;ç®€å•ä»‹ç»&quot;">â€‹</a></h3><p>æ ‡å‡† RAG é’ˆå¯¹å¤§å‹å¼‚æ„è¯­æ–™åº“è®¾è®¡ï¼Œè€Œ agent memory æ˜¯æœ‰ç•Œçš„ã€è¿è´¯çš„å¯¹è¯æµï¼Œå…·æœ‰é«˜åº¦ç›¸å…³ä¸”ç»å¸¸é‡å¤çš„ç‰‡æ®µã€‚xMemory æå‡ºè§£è€¦åˆ°èšåˆçš„æ–°èŒƒå¼ï¼šå°† memory åˆ†è§£ä¸ºè¯­ä¹‰ç»„ä»¶ï¼Œç»„ç»‡æˆå±‚æ¬¡ç»“æ„ï¼Œå¹¶ç”¨è¯¥ç»“æ„é©±åŠ¨æ£€ç´¢ã€‚</p><h3 id="ç›¸å…³å·¥ä½œ-2" tabindex="-1">ç›¸å…³å·¥ä½œ <a class="header-anchor" href="#ç›¸å…³å·¥ä½œ-2" aria-label="Permalink to &quot;ç›¸å…³å·¥ä½œ&quot;">â€‹</a></h3><p>RAG-based memory systemsã€semantic memory decompositionã€hierarchical memory organization</p><h3 id="åˆ›æ–°ç‚¹-2" tabindex="-1">åˆ›æ–°ç‚¹ <a class="header-anchor" href="#åˆ›æ–°ç‚¹-2" aria-label="Permalink to &quot;åˆ›æ–°ç‚¹&quot;">â€‹</a></h3><ul><li>æå‡º &quot;decoupling to aggregation&quot; æ–°èŒƒå¼ï¼Œè¶…è¶Šä¼ ç»Ÿçš„ similarity matching</li><li>xMemory æ„å»º intact units å±‚æ¬¡ç»“æ„ï¼Œé€šè¿‡ sparsity-semantics objective æŒ‡å¯¼ memory split/merge</li><li>Top-down æ£€ç´¢ï¼šä¸º multi-fact queries é€‰æ‹©ç´§å‡‘å¤šæ ·çš„ themes å’Œ semantics</li></ul><h3 id="æ•ˆæœ-2" tabindex="-1">æ•ˆæœ <a class="header-anchor" href="#æ•ˆæœ-2" aria-label="Permalink to &quot;æ•ˆæœ&quot;">â€‹</a></h3><p>åœ¨ LoCoMo å’Œ PerLTQA ä¸Šï¼Œä½¿ç”¨ä¸‰ç§æœ€æ–° LLMs å‡å–å¾—ä¸€è‡´çš„ answer quality å’Œ token efficiency æå‡ã€‚</p><details><summary>æŸ¥çœ‹æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰</summary><p>Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-k similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity-semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader&#39;s uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.</p></details><hr><h2 id="_4-live-evo-online-evolution-of-agentic-memory-from-continuous-feedback" tabindex="-1">[4] Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback <a class="header-anchor" href="#_4-live-evo-online-evolution-of-agentic-memory-from-continuous-feedback" aria-label="Permalink to &quot;[4] Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback&quot;">â€‹</a></h2><p><strong>é“¾æ¥ï¼š</strong> <a href="https://arxiv.org/abs/2602.02369" target="_blank" rel="noreferrer">https://arxiv.org/abs/2602.02369</a></p><p><strong>å‘å¸ƒæ—¶é—´ï¼š</strong> 2026-02-02</p><h3 id="ç®€å•ä»‹ç»-3" tabindex="-1">ç®€å•ä»‹ç» <a class="header-anchor" href="#ç®€å•ä»‹ç»-3" aria-label="Permalink to &quot;ç®€å•ä»‹ç»&quot;">â€‹</a></h3><p>ç°æœ‰ self-evolving memory ç³»ç»Ÿå¤šåŸºäºé™æ€ train/test splits å¼€å‘ï¼Œä»…åœ¨é™æ€ benchmark ä¸Šè¿‘ä¼¼åœ¨çº¿å­¦ä¹ ï¼Œåœ¨çœŸå®åˆ†å¸ƒåç§»å’Œè¿ç»­åé¦ˆä¸‹è¡¨ç°è„†å¼±ã€‚Live-Evo å®ç°åœ¨çº¿è‡ªè¿›åŒ– memory ç³»ç»Ÿï¼Œä»éšæ—¶é—´æµå…¥çš„æ•°æ®æµä¸­å­¦ä¹ ï¼Œé€šè¿‡ Experience Bank å’Œ Meta-Guideline Bank è§£è€¦ &quot;å‘ç”Ÿäº†ä»€ä¹ˆ&quot; å’Œ &quot;å¦‚ä½•ä½¿ç”¨&quot;ã€‚</p><h3 id="ç›¸å…³å·¥ä½œ-3" tabindex="-1">ç›¸å…³å·¥ä½œ <a class="header-anchor" href="#ç›¸å…³å·¥ä½œ-3" aria-label="Permalink to &quot;ç›¸å…³å·¥ä½œ&quot;">â€‹</a></h3><p>Self-evolving agentsã€online learningã€memory evolutionã€continuous learning systems</p><h3 id="åˆ›æ–°ç‚¹-3" tabindex="-1">åˆ›æ–°ç‚¹ <a class="header-anchor" href="#åˆ›æ–°ç‚¹-3" aria-label="Permalink to &quot;åˆ›æ–°ç‚¹&quot;">â€‹</a></h3><ul><li>åœ¨çº¿è‡ªè¿›åŒ–è®°å¿†ç³»ç»Ÿï¼Œå¤„ç†çœŸå®çš„æ•°æ®æµè€Œéé™æ€æ•°æ®é›†</li><li>åŒ bank æ¶æ„ï¼šExperience Bank å­˜å‚¨åŸå§‹ç»éªŒï¼ŒMeta-Guideline Bank ç¼–è¯‘ä»»åŠ¡è‡ªé€‚åº”æŒ‡å—</li><li>ç±»äººç±»è®°å¿†æœºåˆ¶ï¼šå¼ºåŒ–æœ‰ç”¨ç»éªŒï¼Œè¡°å‡è¯¯å¯¼æ€§/é™ˆæ—§ç»éªŒ</li></ul><h3 id="æ•ˆæœ-3" tabindex="-1">æ•ˆæœ <a class="header-anchor" href="#æ•ˆæœ-3" aria-label="Permalink to &quot;æ•ˆæœ&quot;">â€‹</a></h3><p>åœ¨ Prophet Arena benchmark çš„ 10 å‘¨æµ‹è¯•ä¸­ï¼ŒBrier score æå‡ 20.8%ï¼Œå¸‚åœºå›æŠ¥å¢åŠ  12.9%ï¼Œä¸”èƒ½è¿ç§»åˆ° deep-research benchmarksã€‚</p><p>ğŸ“ <strong>ä»£ç ï¼š</strong> <a href="https://github.com/ag2ai/Live-Evo" target="_blank" rel="noreferrer">https://github.com/ag2ai/Live-Evo</a></p><details><summary>æŸ¥çœ‹æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰</summary><p>Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent self-evolving systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce Live-Evo, an online self-evolving memory system that learns from a stream of incoming data over time. Live-Evo decouples what happened from how to use it via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, Live-Evo maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live Prophet Arena benchmark over a 10-week horizon, Live-Evo improves Brier score by 20.8% and increases market returns by 12.9%, while also transferring to deep-research benchmarks with consistent gains over strong baselines.</p></details><hr><h2 id="_5-infmem-learning-system-2-memory-control-for-long-context-agent" tabindex="-1">[5] InfMem: Learning System-2 Memory Control for Long-Context Agent <a class="header-anchor" href="#_5-infmem-learning-system-2-memory-control-for-long-context-agent" aria-label="Permalink to &quot;[5] InfMem: Learning System-2 Memory Control for Long-Context Agent&quot;">â€‹</a></h2><p><strong>é“¾æ¥ï¼š</strong> <a href="https://arxiv.org/abs/2602.02704" target="_blank" rel="noreferrer">https://arxiv.org/abs/2602.02704</a></p><p><strong>å‘å¸ƒæ—¶é—´ï¼š</strong> 2026-02-02</p><h3 id="ç®€å•ä»‹ç»-4" tabindex="-1">ç®€å•ä»‹ç» <a class="header-anchor" href="#ç®€å•ä»‹ç»-4" aria-label="Permalink to &quot;ç®€å•ä»‹ç»&quot;">â€‹</a></h3><p>è¶…é•¿æ–‡æ¡£æ¨ç†éœ€è¦åœ¨ä¸¥æ ¼å†…å­˜é™åˆ¶ä¸‹åˆæˆåˆ†æ•£åœ¨è¿œè·ç¦»ç‰‡æ®µçš„ç¨€ç–è¯æ®ã€‚ç°æœ‰ streaming agents çš„è¢«åŠ¨ memory update ç­–ç•¥å¾€å¾€æ— æ³•ä¿ç•™ multi-hop reasoning æ‰€éœ€çš„ä½æ˜¾è‘—æ€§æ¡¥æ¥è¯æ®ã€‚InfMem æå‡º System-2 é£æ ¼çš„ memory controlï¼Œé€šè¿‡ PreThink-Retrieve-Write åè®®ä¸»åŠ¨ç›‘æ§è¯æ®å……åˆ†æ€§ã€‚</p><h3 id="ç›¸å…³å·¥ä½œ-4" tabindex="-1">ç›¸å…³å·¥ä½œ <a class="header-anchor" href="#ç›¸å…³å·¥ä½œ-4" aria-label="Permalink to &quot;ç›¸å…³å·¥ä½œ&quot;">â€‹</a></h3><p>Long-context LLMsã€streaming document processingã€memory-constrained reasoningã€System-1/System-2 cognition</p><h3 id="åˆ›æ–°ç‚¹-4" tabindex="-1">åˆ›æ–°ç‚¹ <a class="header-anchor" href="#åˆ›æ–°ç‚¹-4" aria-label="Permalink to &quot;åˆ›æ–°ç‚¹&quot;">â€‹</a></h3><ul><li>System-2 style çš„ memory control æœºåˆ¶</li><li>PreThink-Retrieve-Write åè®®ï¼šä¸»åŠ¨ç›‘æ§ã€targeted retrievalã€evidence-aware joint compression</li><li>SFT-to-RL è®­ç»ƒé…æ–¹ï¼Œå°† retrievalã€writing å’Œ stopping decisions ä¸ end-task correctness å¯¹é½</li></ul><h3 id="æ•ˆæœ-4" tabindex="-1">æ•ˆæœ <a class="header-anchor" href="#æ•ˆæœ-4" aria-label="Permalink to &quot;æ•ˆæœ&quot;">â€‹</a></h3><p>åœ¨ 32k-1M tokens çš„è¶…é•¿ QA benchmarks ä¸Šï¼Œç›¸æ¯” MemAgent å¹³å‡ç»å¯¹å‡†ç¡®ç‡æå‡ï¼š</p><ul><li>Qwen3-1.7B: <strong>+10.17</strong></li><li>Qwen3-4B: <strong>+11.84</strong></li><li>Qwen2.5-7B: <strong>+8.23</strong></li></ul><p>æ¨ç†æ—¶é—´å‡å°‘ <strong>3.9Ã—</strong>ï¼ˆæœ€é«˜ <strong>5.1Ã—</strong>ï¼‰</p><details><summary>æŸ¥çœ‹æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰</summary><p>Reasoning over ultra-long documents requires synthesizing sparse evidence scattered across distant segments under strict memory constraints. While streaming agents enable scalable processing, their passive memory update strategy often fails to preserve low-salience bridging evidence required for multi-hop reasoning. We propose InfMem, a control-centric agent that instantiates System-2-style control via a PreThink-Retrieve-Write protocol. InfMem actively monitors evidence sufficiency, performs targeted in-document retrieval, and applies evidence-aware joint compression to update a bounded memory. To ensure reliable control, we introduce a practical SFT-to-RL training recipe that aligns retrieval, writing, and stopping decisions with end-task correctness. On ultra-long QA benchmarks from 32k to 1M tokens, InfMem consistently outperforms MemAgent across backbones. Specifically, InfMem improves average absolute accuracy by +10.17, +11.84, and +8.23 points on Qwen3-1.7B, Qwen3-4B, and Qwen2.5-7B, respectively, while reducing inference time by 3.9Ã— on average (up to 5.1Ã—) via adaptive early stopping.</p></details><hr><h2 id="_6-latentmem-customizing-latent-memory-for-multi-agent-systems" tabindex="-1">[6] LatentMem: Customizing Latent Memory for Multi-Agent Systems <a class="header-anchor" href="#_6-latentmem-customizing-latent-memory-for-multi-agent-systems" aria-label="Permalink to &quot;[6] LatentMem: Customizing Latent Memory for Multi-Agent Systems&quot;">â€‹</a></h2><p><strong>é“¾æ¥ï¼š</strong> <a href="https://arxiv.org/abs/2602.03036" target="_blank" rel="noreferrer">https://arxiv.org/abs/2602.03036</a></p><p><strong>å‘å¸ƒæ—¶é—´ï¼š</strong> 2026-02-03</p><h3 id="ç®€å•ä»‹ç»-5" tabindex="-1">ç®€å•ä»‹ç» <a class="header-anchor" href="#ç®€å•ä»‹ç»-5" aria-label="Permalink to &quot;ç®€å•ä»‹ç»&quot;">â€‹</a></h3><p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ memory è®¾è®¡é¢ä¸´ä¸¤ä¸ªç“¶é¢ˆï¼š(i) ç¼ºä¹ role-aware customization å¯¼è‡´çš„ memory homogenizationï¼Œ(ii) è¿‡åº¦ç»†ç²’åº¦çš„ memory entries å¯¼è‡´çš„ä¿¡æ¯è¿‡è½½ã€‚LatentMem æå‡ºå¯å­¦ä¹ çš„å¤šæ™ºèƒ½ä½“è®°å¿†æ¡†æ¶ï¼Œä»¥ token-efficient çš„æ–¹å¼å®šåˆ¶ agent-specific memoriesã€‚</p><h3 id="ç›¸å…³å·¥ä½œ-5" tabindex="-1">ç›¸å…³å·¥ä½œ <a class="header-anchor" href="#ç›¸å…³å·¥ä½œ-5" aria-label="Permalink to &quot;ç›¸å…³å·¥ä½œ&quot;">â€‹</a></h3><p>Multi-agent systemsã€collective intelligenceã€memory customizationã€role-based memory</p><h3 id="åˆ›æ–°ç‚¹-5" tabindex="-1">åˆ›æ–°ç‚¹ <a class="header-anchor" href="#åˆ›æ–°ç‚¹-5" aria-label="Permalink to &quot;åˆ›æ–°ç‚¹&quot;">â€‹</a></h3><ul><li>Experience bank ä»¥è½»é‡åŒ–å½¢å¼å­˜å‚¨åŸå§‹äº¤äº’è½¨è¿¹</li><li>Memory composer åŸºäºæ£€ç´¢ç»éªŒå’Œ agent-specific contexts åˆæˆç´§å‡‘çš„ latent memories</li><li>Latent Memory Policy Optimization (LMPO) ä¼ æ’­ä»»åŠ¡çº§ä¼˜åŒ–ä¿¡å·</li></ul><h3 id="æ•ˆæœ-5" tabindex="-1">æ•ˆæœ <a class="header-anchor" href="#æ•ˆæœ-5" aria-label="Permalink to &quot;æ•ˆæœ&quot;">â€‹</a></h3><p>åœ¨å¤šæ · benchmarks å’Œä¸»æµ MAS æ¡†æ¶ä¸Šï¼ŒLatentMem ç›¸æ¯” vanilla settings æ€§èƒ½æå‡æœ€é«˜è¾¾ <strong>19.36%</strong>ï¼Œä¸”æŒç»­ä¼˜äºç°æœ‰ memory architecturesï¼Œæ— éœ€ä¿®æ”¹åº•å±‚æ¡†æ¶ã€‚</p><details><summary>æŸ¥çœ‹æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰</summary><p>Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to 19.36% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.</p></details><hr><h2 id="ğŸ“Š-æœ¬å‘¨è¶‹åŠ¿æ€»ç»“-2026-02-03-2026-02-08" tabindex="-1">ğŸ“Š æœ¬å‘¨è¶‹åŠ¿æ€»ç»“ï¼ˆ2026-02-03 ~ 2026-02-08ï¼‰ <a class="header-anchor" href="#ğŸ“Š-æœ¬å‘¨è¶‹åŠ¿æ€»ç»“-2026-02-03-2026-02-08" aria-label="Permalink to &quot;ğŸ“Š æœ¬å‘¨è¶‹åŠ¿æ€»ç»“ï¼ˆ2026-02-03 ~ 2026-02-08ï¼‰&quot;">â€‹</a></h2><table tabindex="0"><thead><tr><th>ç ”ç©¶æ–¹å‘</th><th>è®ºæ–‡æ•°é‡</th><th>æ ¸å¿ƒä¸»é¢˜</th></tr></thead><tbody><tr><td><strong>Runtime Memory Control</strong></td><td>2</td><td>BudgetMem (é¢„ç®—è·¯ç”±)ã€InfMem (System-2 æ§åˆ¶)</td></tr><tr><td><strong>Memory Evolution</strong></td><td>2</td><td>TAME (å¯ä¿¡è¿›åŒ–)ã€Live-Evo (åœ¨çº¿è¿›åŒ–)</td></tr><tr><td><strong>Retrieval Architecture</strong></td><td>1</td><td>xMemory (è§£è€¦-èšåˆèŒƒå¼)</td></tr><tr><td><strong>Multi-Agent Memory</strong></td><td>1</td><td>LatentMem (è§’è‰²å®šåˆ¶åŒ–)</td></tr></tbody></table><h3 id="ğŸ”‘-å…³é”®è¶‹åŠ¿" tabindex="-1">ğŸ”‘ å…³é”®è¶‹åŠ¿ <a class="header-anchor" href="#ğŸ”‘-å…³é”®è¶‹åŠ¿" aria-label="Permalink to &quot;ğŸ”‘ å…³é”®è¶‹åŠ¿&quot;">â€‹</a></h3><ol><li><p><strong>æ˜¾å¼æ§åˆ¶æˆä¸ºç„¦ç‚¹</strong>ï¼šBudgetMem å’Œ InfMem éƒ½å¼ºè°ƒå¯¹ memory çš„ç²¾ç»†åŒ–æ§åˆ¶ï¼Œå‰è€…å…³æ³¨æˆæœ¬-æ€§èƒ½æƒè¡¡ï¼Œåè€…æ¨¡ä»¿ System-2 è®¤çŸ¥</p></li><li><p><strong>åœ¨çº¿/æŒç»­å­¦ä¹ </strong>ï¼šLive-Evo æå‡ºçœŸæ­£åœ¨çº¿çš„ memory evolutionï¼Œè€Œéé™æ€ benchmark ä¸Šçš„ä¼ªåœ¨çº¿å­¦ä¹ </p></li><li><p><strong>è¶…è¶Š RAG</strong>ï¼šxMemory æå‡º retrieval åº”è¯¥è¶…è¶Š similarity matchingï¼Œè½¬å‘è¯­ä¹‰è§£è€¦å’Œå±‚æ¬¡èšåˆ</p></li><li><p><strong>å¤šæ™ºèƒ½ä½“ memory å®šåˆ¶åŒ–</strong>ï¼šLatentMem è§£å†³è§’è‰²æ„ŸçŸ¥ç¼ºå¤±å¯¼è‡´çš„ memory homogenization é—®é¢˜</p></li><li><p><strong>å®‰å…¨æ€§è€ƒé‡</strong>ï¼šTAME å…³æ³¨ memory evolution ä¸­çš„ safety alignment é—®é¢˜ï¼Œæå‡ºå¯ä¿¡è¿›åŒ–æ¡†æ¶</p></li></ol><hr><p><em>æ•°æ®æ›´æ–°æ—¶é—´ï¼š2026-02-08 20:00 GMT+8</em></p>',90)])])}const u=t(n,[["render",i]]);export{h as __pageData,u as default};
